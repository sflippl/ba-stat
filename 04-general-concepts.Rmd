# Description of `tectr` {#concept}

## Effective Explicitness: A perspective of knowledge

Let us revisit  the introduction: specialized functions and default values render R a powerful programming language for statistical analysis. These concepts are so useful because they constitute crystallized knowledge: when the user calls the function `read.csv`, he effectively uses the programmer's knowledge of how to read in csv-files and therefore lightens his own cognitive load. Of course, this also applies if he has written the function himself; in this case, by externalizing the knowledge he needs not think about the specifics anymore. There are therefore two main advantages of externalizing knowledge. Other users benefit from the user's knowledge and the user himself benefits from its automation. 

Normally, these default values cannot be particularly fancy as the user needs to understand under all circumstances what the function yields -- even if this result is not ideal. This is the restriction that will be loosened for `tectr`. The intention behind this is to automate more knowledge.

How does this work? Consider a hypothetical statistician who looks at a dataset. He might begin by plotting the distribution and a few summary statistics of the variable. Simultaneously, he learns the name and the definition of a variable. He might look at relationships between several variables. Thereby, he creates internal knowledge concerning a proper form of the statistical summary or a certain visualization. This internal knowledge accounts for different aspects; for instance, he might determine a better axis title than `v2x_polyarchy` or find it helpful to denote the mean of the variable on the axis. He might also, for instance determine that a line plot is a better fit than a scatter plot in this case or find that a certain variable transformation is helpful.

This knowledge is seldomly notated which leads to a less efficient workflow. After all, this means that while exploring the data, the statistician will need to think about every modification of the plot which deviates from the default value. As a by-product, any non-crucial change will be left out, even though the statistician might actually benefit from a more descriptive axis title. More severely, when he wants to communicate his results, he needs to reproduce the internally held knowledge; if he creates a certain visualization, he needs to modify the axis title, in a table describing the variables he needs to replace the non-descriptive variable names. If he would like an interactive visualization of the statistical analysis, the problem becomes even bigger; in this case, he needs to externalize his knowledge after the fact.

In order to resolve these issues, `tectr` attempts to track these decisions while they are being made. The program does that by asking the user to be *explicit* about his knowledge -- which is usually only represented internally. 

If `tectr` used default values of a similar kind as other functions, this would lead to a severe burden on the user as it is time expensive to externalize knowledge. `tectr` therefore uses more flexible default values that attempt to approximate the statistician's thought process. The user therefore mainly has to intervene when something does not adhere to canonical assumptions. As he is more likely to think about these situations explicitly anyway, the burden might even decrease considering that under usual circumstances, the users will be forced to make these explicit decisions several times.

I will call this concept *effective explicitness* and it is central to `tectr`. To emphasize that, this term guides the semantics of `tectr`: those functions and objects which support effective explicitness will be preceded by `fx` (e**f**fective e**x**plicitness, pronounced: effex).

The notion of explicitness is clear in this context; if the user gains new knowledge that is not represented within the metaframe, he should be explicit about it. This applies to visualizations as well as general observations which he may notate in a comments field. This process should be effective in two ways: firstly, the metaframe needs to imitate the statistical thought process closely enough that many of its approximations are valid; secondly, if adaptations need to be made, they should be scalable. Consider, as an example, the variable year in the previous chapter. The program's assumption that this was an ordinary continuous variable was wrong. In order to correct this, it was sufficient to make explicit that the variable referred to time and tracked the development for different countries. On the other hand, the variable could have been so unique that these little changes would not have sufficed. In these cases, it should be possible to make broader changes. The remaining chapter will present a pilot implementation of this more general philosophy. Before going on to discuss specific functions, I will present the underlying concepts which power these functions. In order to properly discuss the functions, I will focus on the following aspects:

* What do the functions return?

* How does the function allow the user to be explicit?

* What would be alternative implementations?

* How mature is the function?
 
The last point is especially important: as a pilot implementation, `tectr` provides functionality that seems to go in the right direction. However, only a combination of more exhaustive features and applications will determine whether the approach was the right one. It is therefore important to identify those functions where an entirely different track of thought might be necessary in the future. In order to make the description lightweight and not focus too much on the particularities of implementation, I will often refer to the package documentation for further information.
 
## Underlying structure
 
### The metaframe
 
The *metaframe* is the attribute of a database which captures the knowledge of the user. It adheres to the principle of tidy data [@Wickham2014]:
 
 > Each variable forms a column.
 > 
 > Each observation forms a row.
 > 
 > Each type of observational unit forms a table.
 
Clearly, additional information about observations can be stored in an additional column. The metaframe stores additional information about variables by treating every variable as an observation and every type of information as a column. It consists of different protected variable names that fulfill specific functions. The `name` column refers to the corresponding variable name. The remaining information is preceded by a keyword to make clear what the variable refers to. More specifically, those variables that refer to semantic information are preceded by `fxInfo` (e. g. the "proper" name of the variable `fxInfo_name`) and those variables that give information on the visualization are preceded by `fxGeom`.

For details on metaframes, how to set or change them, see the documentation. In my opinion, the best workflow is instantiating the metaframe with `fx_default` and then adding columns via `mutate_mf`. This allows you to only use the data frame itself as a function argument and provides a concise terminology:

```{r eval = FALSE}
data <- 
  tibble(ht = c(158, 189, 178)) %>% 
  fx_default() %>% 
  mutate_mf(fxInfo_name = "Height")
metaframe(data)
```

```{r 1st-mf, echo = FALSE}
data <- 
  tibble(ht = c(158, 189, 178)) %>% 
  fx_default() %>% 
  mutate_mf(fxInfo_name = "Height")
kable(
  metaframe(data), booktabs = TRUE,
  caption = "A simple metaframe"
)
```


### Extending fx functions

#### Dispatch

Whereas in most cases, modifying the parameters to the functions should be sufficient, there are also many applications where a more extensive modification is necessary -- or at least more effective. `tectr` uses method dispatch to coordinate these extension. In certain occasions, S4 classes are necessary. As long as S3 class suffice, they are utilized.

It is important to emphasize that the fx functions themselves are not generics. Internally, every fx function calls an `fxi` (short for fx *internal*) function which modifies its arguments to make dispatch possible [^1]. I will elaborate on the particular mechanism in the following sections. By defining dummy classes, the calls to so called `fxe` (short for fx *extendible*) functions employ dispatch and can be extended. The metaframe information of the current variable is called as spliced input to the `fxe` function. For instance, in the example above, an extendible function `fxe_fun` would have received as additional input the argument `fxInfo_name` and `name`.

[^1]: In some cases, these modifications are so simple that they happen in the fx function and the fxi function is omitted.

#### S3 dummy classes

S3 dummy classes are mainly powered by the function `fxd` which creates a *subclass* for a specific *task*. For instance, the task "default" which specifies the default values for the metaframe columns has as its subclasses the corresponding columns, e. g.:

```{r}
fxd("default", "fxInfo_name")
```

This provides a lightweight and effective mechanism for dispatch via dummy classes.

#### S4 dummy classes

S4 dummy classes are powered by different classes for every task they employ. For instance the `fxGeom` dummy classes all inherit from `fxGeom` and consist of the aforementioned possible classes of input to visualization. Consider, for instance,

```{r}
fxGeom("Continuous")
```

which yields the corresponding S4 class `fxGeomContinuous`.

This mechanism provides a better overview over all possible classes. However, it is necessary to explicitly define every task and every subclass. Normally, the `fxd` mechanism should be preferred. S4 dummy classes are mainly employed when multiple dispatch becomes necessary (see [`fx_ggplot`]).

### Other concepts

I have defined a few other objects that can be useful.

#### Filepaths

The S3 class `filepath` consists of a character vector which contains paths to external data and a `reader` attribute which can read in the data. It allows you to retrieve stored data within a metaframe in an uncomplicated way.

#### fx factors

fx factors are a very simple class which extends the concept of factors from `character`s to arbitrary classes. Certain columns of the metaframe might, for instance, contain different functions many times and it would be a waste to store them separately. `fx_factor` creates an object which has as the attribute levels its unique values and refers to them within every element via the index of the levels. The original object can be recreated via `fx_evaluate`.

## `fx_default`

### General purpose

`fx_default` computes default values for every column of the metaframe so that the different fx functions can fill up the metaframe with the remaining necessary columns. The function therefore uses specific heuristics for the different columns to infer sensible values. As in many cases, the default values are modified, for every column there is a function `fx_default_<colname>` (e. g. `fx_default_fxInfo_name`) which allows the user to access the default values separately from the internal mechanism of `fx_default`.

A call to `fx_default` specifies the columns to be imputed. By default, if the data frame has no metaframe, it creates the metaframe with one observation for every variable name.

### Extensions

Extensions may be provided via S3 method dispatch over the function `fxe_default` and the class `fxd_default_<colname>`. The function may depend on the data and on its metaframe. If possible, it is recommended that the functions only depend on the data frame and the name column of the metaframe. Consider, for instance, the default function for `fxGeom_class`:

```{r}
tectr:::fxe_default.fxd_default_fxGeom_class
```

Internally, it calls the more accessible function `fx_default_fxGeom_class` which does the work (for details, see the documentation).

### Alternative implementations

Like with respect to general functions in R, there are essentially two possible implementations of default values in `tectr`: either the default is specified in the head or the function recognizes that the default is needed if the argument has the value `NULL`. Which mechanism is preferable, depends strongly on the context. It is recommended that parameters that are specific to one function and have a very simple default value (for instance, a constant numerical value) are marked via `NULL` whereas the user may want to access broadly applicable parameters that employ a more elaborate inference mechanism directly. These should therefore be implemented via an `fx_default` function.

### Lifecycle

The purpose of `fx_default` is clearly outlined and its application is very simple. Modifications of the internal dispatch are possible but the extendible functions and `fx_default` itself are unlikely to be reworked.

## `fx_info`

### General purpose

This function provides information on the different variables to the user. Examples for this information would be

* the name of the variable (which is often different from its internal name)
* the description or definition of the variable
* summary statistics.

A call to `fx_info` consists of the data itself, a topic and potentially additional parameters. The *topic* may either be a certain semantic topic like the name or the description or it may invoke a specific method. The most useful example of a topic is "stats" which renders a summary table with descriptive statistics that can be specified in the argument statistics:

```{r}
fx_info(mtcars, "stats", statistics = c("mean", "quantile"))
```

As can be seen the function returns a data frame with a certain information in every column. If no method for the particular topic exists, the function searches the metaframe for a column of the name "fxInfo_<topic>" and returns that. Consider, for instance, a name and a short comment:

```{r}
mtcars %>% 
  select(mpg) %>% 
  fx_default() %>% 
  mutate_mf(
    fxInfo_name = "Miles/(US) gallon", 
    fxInfo_comment = "A gallon is 3.79 litres."
  ) %>% 
  fx_info(c("name", "comment", "stats"), statistics = c("mean", "quantile"))
```

We will later use this functionality to create meaningful descriptive tables.

You may provide either characters or functions to statistics. Characters are evaluated via `do.call`, functions are called directly. If you specify a function, you should allow for arbitrary parameters via `...` as a function will receive the columns of the metaframe as input. A function's first argument should be the corresponding column of the data frame. Consider the following example, in which the number of digits of the computed mean depends on the information in the metaframe:

```{r}
stat_mean <- function(x, ..., fxInfo_digits) 
  x %>% mean() %>% round(digits = fxInfo_digits)
mtcars %>% 
  select(mpg) %>% 
  fx_default() %>% 
  mutate_mf(
    fxInfo_name = "Miles/(US) gallon", 
    fxInfo_digits = 3
  ) %>% 
  fx_info(c("name", "stats"), 
          statistics = list(mean = stat_mean))
```


### Extensions

Extensions can be provided via dispatch over `fxe_info`. The dummy class for a certain topic will be provided by `fxd("info", <topic>)`, e. g.

```{r}
fxd("info", "stats")
```

### Lifecycle

In my opinion, the combination of simplicity and power makes `fx_info` a very useful function that will most likely retain its structure. This means that new functions which enhance the power of `fx_info` and especially its stats will be the next step in its development.

## `fx_output`

### General purpose

This function makes `fx_info` immediately applicable in a wide variety of applications. It transforms the data frame returned by `fx_info` into a certain *form*. In this case, a sparse wrapper around a function from another package often suffices. `fx_info` may return different formats. Currently, the supported formats are rst, html, latex and markdown. The two most useful forms that have been implemented so far are *table* and *collapse*.

As an example, we will consider the mtcars summary statistics:

```{r}
stat_median <- function(x, ...) 
  x %>% median() %>% round(digits = 3)
stat_mean <- function(x, ...)
  x %>% mean() %>% round(digits = 3)
info <- 
  mtcars %>% 
  fx_info("stats", list(mean = stat_mean, median = stat_median))
info
```

```{r}
fx_output(info, form = "table", out_format = "markdown")
```

The form "collapse", on the other hand yields one row per observation. This row consists of different cells in which the variable information and its name are listed in a specified format and which are then glued together:

```{r}
fx_output(info, form = "collapse")
```

These specifications can also be modified:

```{r}
fx_output(info, form = "collapse", cell_scheme = "The {name} is {value}.", cell_sep = " --- ")
```

Further information can be found in the documentation.

### Extensions

`fx_output` may be extended via the `fxd` dummy class with the topic "output".

### Lifecycle

`fx_output` is stable and the next step will be to refine and extend the output forms. In particular, the next version will provide an output form "report" which creates a dynamical report of the kind which we will render in the [fifth chapter](#application).

## `fx_write`

### General purpose

Besides providing summary tables, the main purpose of `fx_info` is to make information about the variable accessible. The foundation of this information can often be imported from a codebook of some form (see the [application]). Adding and editing this information -- e. g. correcting typos -- is, however, an awkward endeavour in R. A text editor of some sort would be more suitable. The `fx_write` family of functions attempts to adress this by allowing you to export the semantic data to human-readable documents which can then be modified. As it returns the data frame with a `filepath` column which refers to the export file, it can easily be read in -- this functionality is provided by `fx_read`. 

Currently, only one `fx_write` function is implemented: `fx_write_json`. This function creates several files in JavaScript Object Notation which is well readable by humans. The function is powered by the package `jsonlite` [@jsonlite] which uses a class based mapping to convert between JSON data and R objects. For details, see the documentation and the fifth chapter.

### Extensions

While `fx_write` does not employ any kind of dispatch but is a family of functions, it can be extended (in a looser definition of the word) by providing a new method with a consistent interface `fx_write_<format>`.

### Alternative implementations

`fx_write` is currently an experimental feature which has its disadvantages. Most notably, it is difficult to change the codebook import after somebody has changed the output of this process (i. e. the exported files). While version control would make this simpler, the fact remains that all changes to the document would have to be manually reentered. A primitive editor within R which tracks changes would render the process more reproducible and less complicated. However, this would be a considerable effort with limited applicability and for the time being, it seems to me that a method as represented by `fx_write_json` (or possibly another format) represents the best alternative.

### Lifecycle

This function is at the moment very unstable and should be used with caution.

## `fx_ggplot` {#fx-ggplot}

At last, I present the fx function which has the most extensive implementation so far: `fx_ggplot`, which is concerned with the visualization of variables. This functions makes a more extensive introduction necessary. The first section will discuss the general concept behind `fx_ggplot`. We will then discuss the implementation in `tectr` and how it may be extended. The fourth section discusses possible alternatives and the fifth section elaborates how development of the function will progress.

### Concept of `fx_ggplot`

This function intends to provide a flexible visualization. As an input, it accepts aesthetics and then attempts to produce the best visualization that is compatible with the specified aesthetics. Therefore, it does not answer the question "What should I know about these five variables?". More specifically, it does not prevent you from overplotting, nonsensical aesthetics etc. In my opinion, it is best applied by a person who has an idea of the expected results. In this case, it is especially adept at flexible, interactive visualizations (see below). For instance, if a statistician is interested in several time series of different variables, these should be differently visualized depending on the available information.

While I will discuss this concept below, I will assume, for now, that this method is valid and consider the following question: How do we infer the best visualization from a given set of aesthetics? As the function is created within the syntax of `ggplot2`, our task is to infer the set of elements which constructs the plot. These elements belong to two categories: *dependent* and *independent*.

**Independent** elements will be added for a certain specified aesthetics regardless of the other aesthetics, e. g. the title of the x axis. This title only depends on the x aesthetic. The y aesthetic has no influence on this layer.

On the other hand, the y aesthetic has a big influence on the question what geometry we should add, as the suitable geometry depends on all aesthetics. Such elements are therefore called **dependent**.

The determination of independent elements is relatively clear and simple; it only depends on one aesthetic and it is therefore easy to define a function which depends on certain parameters of the variable and yields a layer. On the other hand, the determination of dependent layers is messier, as we will see below.

For that purpose, I will first present the original concept I had implemented for dependent layers: a strictly rule-based structure which specifies, for any combination of relevant aesthetics which layers this would yield. For instance, this structure included:

* continuous x-variable and continuous y-variable: `geom_point()`
* discrete x-variable and continuous y-variable: `geom_boxplot()`

The basic plots are easily produced with such a system but whereas it allows the user to be very explicit, it is very difficult to implement a meaningful influence via parameters. Instead, any new kind of visualization made necessary an entirely new class with a wide variety of methods. Furthermore, despite the seeming clarity of these simple rules, the resulting massive dispatch was likely to end in a lot of confusion and an unmanageable code base.

I have therefore developed an alternative approach which implements a *voting system* for the dependent layers. (In contrast, I will refer to my initial idea as a rule-based system.) As a brief overview, the voting system consists of three phases:

* In the **nomination phase**, every variable nominates possible sets of layers which would make sense with this variable. These nominations depend on the aesthetic, the `fxGeom_class` and possibly the metaframe parameters.
* In the **veto phase**, every variable receives the entire list of nominations and removes those, it is incompatible with.
* In the **voting phase**, every variable distributes a number of votes on the various remaining nominations. These votes depend on properties of the nominations. The nomination with the most votes is chosen as the winner and yields the dependent layers.

This approach may appear unconventional, at first, and I will adress some counterarguments below. But before that, I will present the implementation and process of extension for both the independent and the dependent layers.

I will introduce the implementation with a simple example from the diamonds dataset: we will consider as x aesthetic the weight in carat and as y aesthetic the price. In the end, we will have created figure \@ref(fig:bsp)

```{r bsp, fig.cap = "(ref:bsp)", out.width = '50%', fig.align = 'center'}
fx_ggplot(diamonds, aes(x = carat, y = price)) + 
  my_theme
```
(ref:bsp) Example for `fx_ggplot`

The first step is to infer the default values of the metaframe. All required columns for `fx_ggplot` are stored in the vector `fx_ggplot_columns`:

```{r eval = FALSE}
fx_diamonds <- 
  diamonds %>% 
  select(carat, price) %>% 
  fx_default(columns = fx_ggplot_columns)
metaframe(fx_diamonds)
```

```{r fx-diamonds, echo = FALSE}
fx_diamonds <- 
  diamonds %>% 
  select(carat, price) %>% 
  fx_default(columns = fx_ggplot_columns)
kable(
  metaframe(fx_diamonds), booktabs = TRUE, 
  caption = "(ref:fx-diamonds)"
)
```

(ref:fx-diamonds) The default metaframe for `fx_ggplot`.

### Implementation of the independent layers

The independent layers are determined by the internal function `fxi_layer_single` which calls `fxe_layer_single` for every aesthetic. This function dispatches over `fx_geom`, the argument that is created by the `fxGeom_class` column. Therefore, the function is once called for the class `fxGeomContinuous` and `xAesName` and once for the class `fxGeomContinuous` and `yAesName`. The data frame itself is passed to the extendible function as well. The default method will simply concatenate the results of two underlying functions: `fxe_layer_scale` and `fxe_layer_other`, of which the former is usually more important. `fxe_layer_scale` depends on almost all arguments (each with the prefix "fxGeom_") which can be provided to a scale in `ggplot2`. If none are specified the default values will be used. Normally, at least the transformation and the limits will be inferred by `fx_default`. In this case, the limits correspond to the range of the data and therefore to the default of the scale, as well. However, both the x and the y scale are logarithmized in this case.

Whereas the remaining scale arguments are relatively evident and may be looked up in the documentation, automatically determining the transformation is a bit more complex. It is inferred by the function `fx_default_fxGeom_trans` which depends on two additional parameters: `fxGeom_trans_simple`, a boolean, and `fxGeom_trans_p.threshold`, a numerical value between 0 and 1. `fxGeom_trans_simple` determines how complicated the transformations are allowed to be. If it is true (the default), the function only chooses between three transformations: identity, square root and log. It determines the variable's skewness as implemented in the `moments` package [@moments] and chooses the transformation with the least absolute skewness. To ensure well-definedness, this only applies to positive values. In the case of nonpositive values, the identity is chosen by default. On an exploratory basis, this rule yielded good results. In the case of the weight and price of diamonds, a log-transformed scale is a sensible choice, as well.

The aforementioned three transformations are special cases of the more general *boxcox-transformations* which have first been proposed by G. E. P. Box and D. R. Cox [-@Box1964]. These depend on the parameter $\lambda>=0$ and take the following form:

$$y^{(\lambda)}=\begin{cases}\frac{y^{\lambda}-1}{\lambda}&\text{ if }\lambda > 0\\\ln y&\text{ if }\lambda = 0\end{cases}$$

Thus, the log transformation is given for $\lambda=0$, the square root transformation by $\lambda =0.5$ and the identity by $\lambda=1$.[^2] These transformation can now be further generalized to allow that $y$ is previously shifted by an offset in order to adapt the transformation to negative values, as well. This *boxcox-transformation with offset* is employed by the non-simple inference mechanism (i. e. `fxGeom_trans_simple = FALSE`.

[^2]: More precisely, they are affine transformations of these functions and scale transformations are invariant in affine functions.

The function first uses the Agostino-test as implemented in `moments` as a heuristic whether the data is skewed. If the p-value of the test is below the threshold given by the parameter `fxGeom_trans_p.threshold`, it fits $\lambda$ and the offset with the help of the function `boxcoxfit` from the package `geoR` [@geoR]. The resulting transformation is used for the corresponding scale.

While the latter method is more sensitive to the patterns in the data, it has several disadvantages: firstly, the Agostino-test is flawed as a heuristic as, for large datasets, it essentially always denies the null hypothesis, even if the data is only slightly skewed and the identity transformation is still suitable. These leads to many general boxcox transformation which do not adhere to common first analyses whereas a log- or square root-scale is employed more frequently. Finally, in some instances, the boxcoxfit function does not converge resulting in an error.

These are the reasons why the simple transformation inference is set as the default.

Due to the large skewness, both scales therefore return a scale with a log transformation.

It might also be interesting to see that `fx_info` is applied at this stage - this is how the number of missing values was displayed in the plots in chapter three. The relevant topic is the "title" which can be modified by parameters such a `fxInfo_title_na.show` or, more generally, `fxInfo_title_stats`. Consider the following example with modified statistics and a unit on the y axis.

```{r eval = FALSE}
fx_diamonds <- 
  diamonds %>% 
  select(carat, price) %>% 
  fx_default(columns = fx_ggplot_columns) %>% 
  mutate_mf(fxInfo_title_na.show = name == "price", 
            fxInfo_title_n.show = TRUE, 
            fxInfo_title_stats = "mean", 
            fxInfo_unit = purrr::map(name, ~ if(. == "price") "$" else NULL), 
            fxInfo_title_unit.show = TRUE)
metaframe(fx_diamonds) %>% 
  select(name, starts_with("fxInfo"))
```

```{r title-ex, echo = FALSE}
fx_diamonds <- 
  diamonds %>% 
  select(carat, price) %>% 
  fx_default(columns = fx_ggplot_columns) %>% 
  mutate_mf(fxInfo_title_na.show = name == "price", 
            fxInfo_title_n.show = TRUE, 
            fxInfo_title_stats = "mean", 
            fxInfo_unit = purrr::map(name, ~ if(. == "price") "$" else NULL), 
            fxInfo_title_unit.show = TRUE)
fx_diamonds %>% 
  metaframe() %>% 
  select(name, starts_with("fxInfo")) %>% 
  kable(
    booktabs = TRUE, 
    caption = "Metaframe of diamonds with modified parameters"
  )
```


This modified metaframe results in the following plot:

```{r title, fig.cap = "(ref:title)", out.width='70%', fig.align='center'}
fx_ggplot(fx_diamonds, aes(x = carat, y = price)) + 
  my_theme
```
(ref:title) Visualization with metaframe parameters as in table \@ref(tab:title-ex)

I refer to the documentation for more details.

Another independent layer function is given by `fxi_labeller` which provides the labeller for a facetting variable. Facetting variables are specified in the argument `facet_vars` and need to be captured by the function `vars()`. An example can be seen in figure \@ref(fig:facets)

```{r facets, fig.cap='(ref:facets)', out.width='70%', fig.align='center'}
diamonds %>% 
  filter(color %in% LETTERS[4:6]) %>% 
  fx_ggplot(aes(x = carat, y = price), facet_vars = vars(color)) + 
  my_theme
```
(ref:facets) Facetting in `fx_ggplot`

### Implementation of the voting system

#### Nomination

The dependent layers are determined by the internal function `fxi_layer_complete`. The nominations are handled by `fxe_layer_complete_nominate`, which is dispatched over `fx_geom` and `aes_name`, as well. The function accepts the parameter `fxGeom_nominations` which may provide additional nominations. As these are absent, only the default nominations are provided.

These are handled by the S3 class `nominations` which returns, depending on the access function, all layers, scales, facets, coordinate system or other `ggproto` objects within a certain nomination (see the documentation). This will become important for votes and vetos.

The nominations by the x aesthetic can be caught by

```{r}
x_noms <- fxe_layer_complete_nominate(fxGeom("Continuous"), AesName("x"), diamonds)
```

As such an object is messy to print, I will simply list the four nominations:

* `geom_point()` with an automatically determined transparency value `alpha`
* `geom_histogram()`
* `geom_density()`
* `geom_hex(), scale_fill_gradient()` with limits which are anchored at 0 and a custom palette.

Evidently, these layers are suitable for very different plots. If you consider which layers to nominate, you can simply choose all those that make sense for any combination that includes this aesthetic and `fxGeom_class`. Votes and vetos will take care of the rest.

Besides supplying additional nominations via `fxGeom_nominations`, you can extend the function by importing the S4 generic and defining a new method.

#### Vetoes

Going on, the function `fxe_layer_complete_veto` is responsible for the vetoes. Its structure is

```
fxe_layer_complete_veto(nomination, fx_geom, aes_name, data, ..., fxGeom_veto = NULL)
```

and it dispatches over `fx_geom` and `aes_name`. `fxGeom_veto` can specify a function which vetoes additional nominations. The internal function calls the appopriate method for every nomination. The method simply returns a boolean where `FALSE` causes no action and `TRUE` indicates that the nomination ought to be removed. Consider, for instance, the call below:

```{r}
fxe_layer_complete_veto(nomination(geom_density()), fxGeom("Continuous"), AesName("y"), diamonds)
```

This aesthetic vetoes a density plot because it is incompatible with an existing y aesthetic. Consequently, only the scatter plot and the hexagonal heatmap remain as valid options (all other nominations from the y aesthetic have been vetoed, as well).

#### Votes

Finally, the following function casts the votes:

```
fxe_layer_complete_vote(nomination, fx_geom, aes_name, data, ..., fxGeom_vote)
```

`fxGeom_vote` can specify a function which may cast additional votes. `fxe_layer_complete_vote` dispatches over the same arguments and returns, for a given nomination, an integer which represents the votes for a certain nomination. In this case, we observe:

```{r}
fxe_layer_complete_vote(nomination(geom_point()), fxGeom("Continuous"), AesName("x"), diamonds)
fxe_layer_complete_vote(nomination(geom_hex()), fxGeom("Continuous"), AesName("x"), diamonds)
```

The same applies to the y aesthetic. Why does the heatmap receive more votes? The reason lies in the large amount of data -- the threshold at which the heatmap receives the majority of votes can be determined by the parameter `fxGeom_hex.threshold` and its default is set to 1000.

The hexagonal heatmap therefore receives 4 votes whereas its only rival, the scatter plot, receives 0 votes. From these steps, the above pictured plot emerges.

### Discussion and alternatives

This subsection discusses the implementation of `fx_ggplot`, lists advantages and adresses possible disadvantages.

#### Easy extendibility

The partition of the `fx_ggplot` functionality into these partial functions allows the user to build a complex visualization by many very simple steps; he only needs to think about one scale, one possible (resp. impossible) visualization and so on at a time. If a visualization does not correspond to the user's need, it is simple enough to fix it; she needs not define an entire new class but only modify certain parameters and if the need for a new class arises, its definition is both shorter and easier.
This corresponds to the guideline of `tectr` that it should be easy to integrate into the common explorative process of a statistician.

#### High flexibility

This advantage is as much hypothesis as observation. The aforementioned easily acquired extension will also easily work with different classes that have not actually been intended to work together -- they may even have been specified by different persons. In a rule-based process, it is not possible to achieve this kind of flexibility, as far as I can see it. 

#### What about intransparency?

A possible argument against the voting system might be the following:

> It is very intransparent what arguments will result in which plot. All these different functions occlude what could better be solved by an independent and a dependent element function where the independent element function is simply a rule-based process.
>
> The amount of time such detailedness would require is easily exceeded by the follow-up process regarding an unsuitable visualization in the voting system. The user then needs to consider three different functions and identify a suitable solution which he must validate by some kind of testing. Finally, even when he has found a solution, changes in the functions of another aesthetic might result in an unsuitable plot once again.
>
> The voting system arises from laziness in the wrong domain and requires a higher effort than a rule-based system for a coequal result.

As applies to most arguments with regards to programs in such early stages of development, the most reliable answer would be to wait and see which system works best. However, I would argue in favor of a few heuristics that are on my side. Firstly, I would argue that the amount of time for such an adaptation is limited, as the user can basically follow a -step program: let us assume that the dependent elements x are shown but the user would like to see the elements y.

1. See if y is nominated. If it is not, nominate y and see if the plot changes.
2. See if y is vetoed. If it is, reconsider either the veto or your own preferences.
3. See if x should be vetoed. If it should, implement the veto and see if the plot changes.
4. See where x and y gather their vote. Change the voting patterns at the appropriate place.

At any step, it might become useful to define a new class. However, this class would be quickly implemented because the user could start with few definitions whereas the rigidity of the rule-based system appears to me more vulnerable to modifications.

This also applies to the second part of the argument. Of course, changes might affect certain plots. However, such a negative change is, in my estimate, more likely in a rule-based system, as the voting system cuts the function some slack and allows the user to demonstrate the importance of a certain nomination in a flexible manner.

#### What about numbers?

This is, of course, a good starting point for a new criticism. It is actually a special case of the intransparency argument:

> Even if these numbers, if applied properly, support a useful visualization system, it is too hard to apply them properly. Humans are not good at handling arbitrary numbers and in a complex system, the effect of these numbers is completely intransparent.

I concede that the introduction of the votes is slightly artificial. I would argue, however, that the proper remedy is not admitting defeat or employing a rule-based system but rather to make these numbers less arbitrary by agreeing on certain standards. After a few application, analysis of the resulting problems might yield a good guideline. Complementary to such an approach, an automated system might be able to compensate for human inability. For instance, an automate system might infer appropriate voting patterns by letting the statistician rank different plots which correspond to certain specified aesthetics.

#### What about letting it go?

> I agree that the voting system appears more suitable for automatic visualization than a rule-based process. Regardless, these automatic visualizations are a fruitless endeavour. The independent elements yield a good adpatation of the plot but the dependent elements adress a problem in a very complicated manner that does not arise as much.
>
> In most cases there are only a few different kinds of plots that are employed and it is easier to define them directly and from case to case manually. Such a definition would be improved by integrating independent elements.

Again, the most reliable response to this is to implement both methods, apply them and compare the results. However, I would like to raise a different point at this place: the definition of the dependent elements could very well occur with the aim of building a few kinds of visualizations. This might even be a good starting point for a heuristic. After building such a model, whenever the need of a new visualization arises it should become increasingly simple to adapt the system such that the new combinations fit together. Therefore, the situation that has been described in the counterargument might also serve as a suitable situation for the implementation of a voting system.

Nevertheless, the answer to this counter-argument needs not be binary. We might very well employ dependent elements with voting system in certain contexts whereas a simpler solution might be suitable in other applications.

### Lifecycle and further development

As I have repeatedly remarked in the previous paragraphs, I believe further validation of the current approach to be crucial. Thus, `fx_ggplot` in its current implementation is potentially useful but far from stable. In particular, the internal nomination process and investigation of the plot elements are a bit awkward. A proper implementation within the `ggproto` system of `ggplot2` might alleviate these problems. However, determining this need and a suitable implementation requires more practical experience with the voting system.

I will therefore apply the voting system to more databases to determine its strengths and weaknesses. In particular, I will consider possible voting pattern standards and a more suitable implementations of the internals. Another important question in this regard has already been raised in the last chapter: perhaps, aesthetics are not optimally suited to specify the desired plot. For instance, the question whether a certain variable is represented by fill or colour is largely dependent on the layer. Perhaps another mapping would provide better results.